use anyhow::Result;
use serde_json::json;
use std::fs;
use tempfile::TempDir;

// Import from the main crate
use deepseek_viewer::search::SearchEngine;
use deepseek_viewer::indexer;

#[tokio::test]
async fn test_ngram_substring_search() -> Result<()> {
    // Create temporary directories for test
    let temp_dir = TempDir::new()?;
    let index_path = temp_dir.path().join("test_index");
    let conversations_path = temp_dir.path().join("conversations.json");
    
    // Create test data with various conversations
    let test_data = json!([
        {
            "id": "1",
            "title": "–û –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏–∏",
            "inserted_at": "2024-01-01T00:00:00Z",
            "mapping": {
                "root": {
                    "children": ["msg1"]
                },
                "msg1": {
                    "message": {
                        "fragments": [
                            {"type": "text", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è –∏ –∫–∞–∫ –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç?"}
                        ]
                    },
                    "children": []
                }
            }
        },
        {
            "id": "2",
            "title": "–ü—Ä–∞–≤–∏–ª–∞ –ø–æ–∏—Å–∫–∞",
            "inserted_at": "2024-01-02T00:00:00Z",
            "mapping": {
                "root": {
                    "children": ["msg2"]
                },
                "msg2": {
                    "message": {
                        "fragments": [
                            {"type": "text", "content": "–ü—Ä–∞–≤–∏–ª–∞ —Ä–∞–±–æ—Ç—ã —Å ngram –ø–æ–∏—Å–∫–æ–º –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—ã–µ"}
                        ]
                    },
                    "children": []
                }
            }
        },
        {
            "id": "3",
            "title": "–§–æ—Ä–º—É–ª—ã –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏",
            "inserted_at": "2024-01-03T00:00:00Z",
            "mapping": {
                "root": {
                    "children": ["msg3"]
                },
                "msg3": {
                    "message": {
                        "fragments": [
                            {"type": "text", "content": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö"}
                        ]
                    },
                    "children": []
                }
            }
        }
    ]);
    
    // Write test data to file
    fs::write(&conversations_path, test_data.to_string())?;
    
    // Build index
    indexer::build_index(
        conversations_path.to_str().unwrap(),
        index_path.to_str().unwrap()
    ).await?;
    
    // Create search engine
    let search = SearchEngine::new(index_path.to_str().unwrap())?;
    
    // Test 1: "–≥—Ä–∞" should find "–≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è"
    let results = search.search("–≥—Ä–∞", 10)?;
    assert!(!results.is_empty(), "Should find results for '–≥—Ä–∞'");
    assert!(
        results.iter().any(|r| r.title.contains("–≥—Ä–∞–≤–∏—Ç–∞—Ü") || r.snippet.contains("–≥—Ä–∞–≤–∏—Ç–∞—Ü")),
        "Should find conversation about –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è with query '–≥—Ä–∞'"
    );
    
    // Test 2: "—Ä–∞–≤" should find both "–≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è" and "–ø—Ä–∞–≤–∏–ª–∞"
    let results = search.search("—Ä–∞–≤", 10)?;
    assert!(results.len() >= 2, "Should find at least 2 results for '—Ä–∞–≤'");
    
    // Test 3: "—Ñ–æ—Ä–º" should find "—Ñ–æ—Ä–º—É–ª—ã" and "–ø–ª–∞—Ç—Ñ–æ—Ä–º"
    let results = search.search("—Ñ–æ—Ä–º", 10)?;
    assert!(!results.is_empty(), "Should find results for '—Ñ–æ—Ä–º'");
    assert!(
        results.iter().any(|r| r.title.contains("–§–æ—Ä–º—É–ª") || r.snippet.contains("—Ñ–æ—Ä–º—É–ª")),
        "Should find conversation about —Ñ–æ—Ä–º—É–ª—ã with query '—Ñ–æ—Ä–º'"
    );
    
    // Test 4: "–ø–æ–∏" should find "–ø–æ–∏—Å–∫"
    let results = search.search("–ø–æ–∏", 10)?;
    assert!(!results.is_empty(), "Should find results for '–ø–æ–∏'");
    assert!(
        results.iter().any(|r| r.snippet.contains("–ø–æ–∏—Å–∫")),
        "Should find text containing '–ø–æ–∏—Å–∫' with query '–ø–æ–∏'"
    );
    
    // Test 5: Too short query (less than 3 chars) - should still work but might not find anything
    let results = search.search("–≥—Ä", 10);
    // This might fail or return empty, which is expected for ngram(3,10)
    assert!(results.is_ok(), "Should handle short queries gracefully");
    
    // Test 6: Multi-word search
    let results = search.search("—Ñ–æ—Ä–º –º–∞—Ç", 10)?;
    assert!(!results.is_empty(), "Should find results for multi-word query");
    
    Ok(())
}

#[tokio::test]
async fn test_search_returns_snippets() -> Result<()> {
    let temp_dir = TempDir::new()?;
    let index_path = temp_dir.path().join("test_index");
    let conversations_path = temp_dir.path().join("conversations.json");
    
    let test_data = json!([
        {
            "id": "1",
            "title": "–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
            "inserted_at": "2024-01-01T00:00:00Z",
            "mapping": {
                "root": {
                    "children": ["msg1"]
                },
                "msg1": {
                    "message": {
                        "fragments": [
                            {
                                "type": "text",
                                "content": "–ê".repeat(300) + "–≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è" + &"–ë".repeat(300)
                            }
                        ]
                    },
                    "children": []
                }
            }
        }
    ]);
    
    fs::write(&conversations_path, test_data.to_string())?;
    indexer::build_index(
        conversations_path.to_str().unwrap(),
        index_path.to_str().unwrap()
    ).await?;
    
    let search = SearchEngine::new(index_path.to_str().unwrap())?;
    let results = search.search("–≥—Ä–∞–≤", 10)?;
    
    assert!(!results.is_empty(), "Should find results");
    
    // Check that snippet is truncated and ends with "..."
    let snippet = &results[0].snippet;
    // Snippet should be ~200 chars (not bytes!) + "..." = 203 chars max
    let char_count = snippet.chars().count();
    assert!(char_count <= 210, "Snippet should be truncated to ~200 chars, got {}", char_count);
    if char_count > 200 {
        assert!(snippet.ends_with("..."), "Long snippet should end with '...'");
    }
    
    Ok(())
}

#[tokio::test]
async fn test_utf8_safety() -> Result<()> {
    let temp_dir = TempDir::new()?;
    let index_path = temp_dir.path().join("test_index");
    let conversations_path = temp_dir.path().join("conversations.json");
    
    // Test with various UTF-8 characters
    let test_data = json!([
        {
            "id": "1",
            "title": "UTF-8 —Ç–µ—Å—Ç ÊµãËØï üöÄ",
            "inserted_at": "2024-01-01T00:00:00Z",
            "mapping": {
                "root": {
                    "children": ["msg1"]
                },
                "msg1": {
                    "message": {
                        "fragments": [
                            {
                                "type": "text",
                                "content": "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! Hello world! ‰Ω†Â•Ω‰∏ñÁïåÔºÅ –≠–º–æ–¥–∑–∏: ü¶Ä‚ö°üîç"
                            }
                        ]
                    },
                    "children": []
                }
            }
        }
    ]);
    
    fs::write(&conversations_path, test_data.to_string())?;
    indexer::build_index(
        conversations_path.to_str().unwrap(),
        index_path.to_str().unwrap()
    ).await?;
    
    let search = SearchEngine::new(index_path.to_str().unwrap())?;
    
    // This should not panic with "byte index is not a char boundary"
    let results = search.search("–ø—Ä–∏", 10)?;
    assert!(!results.is_empty(), "Should find UTF-8 text");
    
    // Snippet should be properly truncated without panicking
    for result in results {
        assert!(result.snippet.len() > 0, "Snippet should not be empty");
    }
    
    Ok(())
}

